##~~ VERSION V12 ~~##
## 15/11/2022

# Changelog:
# Réparation tableau Sendgrid
# Multiple PJ Sendgrid
# Verification présence archive_log ?


## ~~ Imports ~~ ##

## Mailing

import datetime

import base64
import os
from sendgrid import SendGridAPIClient
from sendgrid.helpers.mail import (Mail, Attachment, FileContent, FileName, FileType, Disposition)

today = datetime.date.today()

## ~~ Fonction de scraping ~~ ## 

## Imports
import pandas as pd
import numpy as np
from bs4 import BeautifulSoup
import requests

## Imports Selenium / geckodriver

import urllib.request
from selenium import webdriver
from selenium.webdriver.firefox.options import Options
from selenium.webdriver.common.by import By
import time

## Initialisation driver :

options =  Options()
options.add_argument('--headless')
driver = webdriver.Firefox(options = options, executable_path = "/home/projet_imdb/geckodriver")


## Fonction cleanup pour scraping Subscene
def cleanup(astring):
    bstring = astring.replace(" ","")
    cstring = bstring.replace("\t","")
    dstring = cstring.replace("\n","")
    return(str(dstring))
    
## Fonction de validation de l'étape dans les logs
def validation_update(etape):
    
    columns_logs = ['date','scraping_subscene','parsing_subscene','filtres_cam','url_title','scraping_url_imdb','scraping_imdb_name','scraping_imdb_genre','scraping_imdb_duration','scraping_imdb_rating','film_push','push_archive','email_sent']
    
    if str(etape) not in columns_logs:
        message_sortie = "Etape invalide dans la validation"
        return message_sortie
    
    else:
        archive_log_temp = pd.read_csv(filepath_or_buffer= "archive_log.csv", sep=";", index_col= None, usecols=columns_logs)
        
        date_exec = date.today().strftime('%d/%m/%Y')
        
        last_index = archive_log_temp.index[archive_log_temp['date']==date_exec].tolist()[0]

        archive_log_temp.at[last_index,str(etape)]= int("1")

        archive_log_temp.to_csv(path_or_buf= "archive_log.csv", sep = ";", index = False)
        
        message_sortie = "Etape validée"
        return message_sortie


def scrap_all():
    ## SUBSCENE ##

    individual_url = ""
    liste_url = []
    pattern_sub = "/subtitles/"
    langue = "/english/"
    url_title = pd.DataFrame(columns =['Titre','Url'])
    prefix_url = "https://subscene.com"
    liste_liens_imdb = []
    liste_liens_subscene = []
    
    liste_type = []
    liste_titre = []

    ## Scraping via webscraping.ia:
    
    # url = "https://subscene.com/browse/popular/film/1"
    
    conn = http.client.HTTPSConnection("api.webscraping.ai")

    conn.request("GET", "/html?api_key=****&url=https://subscene.com/browse/popular/film/1")

    res = conn.getresponse()
    data = res.read()

    data_d = data.decode("utf-8")

    soup_true = BeautifulSoup(data_d, 'html.parser')
    contenu_film_type = soup_true.find_all('td', {'class':'a3'})

    ## Etape 1 done : scraping_subscene
    validation_update("scraping_subscene")

    for element in contenu_film_type:
        str_element = str(element)
        type_film = cleanup(str_element)
        start = type_film.find('<tdclass="a3">') +14
        end = type_film.find('</td>', start)    
        element_text = type_film[start:end]

        liste_type.append(element_text)

    contenu_film_title= soup_true.find_all('td', {'class':'a1'})

    ## Etape 2 done : parsing_subscene
    validation_update("parsing_subscene")

    for element in contenu_film_title:
        text = str(element)
        start = text.find('<a href="') + 9
        end = text.find('">\n<div class="visited">')
        liste_titre.append(text[start:end])

    df_a_trier = pd.DataFrame(list(zip(liste_titre,liste_type)), columns = ['Film','Type'])
    df_nocam = df_a_trier[df_a_trier['Type'] != 'Cam']
    df_nocam = df_a_trier[df_a_trier['Type'] != 'Telesync']
    df_nocam_english = df_nocam[df_nocam['Film'].str.contains("/english/")]

    ## Etape 3 done : filtres_cam
    validation_update("filtres_cam")
    
    for each in df_nocam_english['Film']:
        url_each = each
        titre_each = each.split("/subtitles/")[1].split('/english/')[0]
        new_value = {'Titre' : titre_each, 'Url' : prefix_url + url_each}
        url_title = url_title.append(new_value, ignore_index=True)
    
    ## Etape 4 done : url_title
    validation_update("url_title")
    
    ## Suppression duplicates
    url_title.drop_duplicates(subset = 'Titre',inplace=True)
    url_title.reset_index(drop = True, inplace = True)
    
    ## Récupération des URL IMDB :

    for each_url in url_title['Url']:
        #html_subscene = requests.get(each_url)
        conn = http.client.HTTPSConnection("api.webscraping.ai")

        conn.request("GET", "/html?api_key=****=" + str(each_url)   )

        res = conn.getresponse()
        data_subsc = res.read()
        data_subsc_d = data_subsc.decode("utf-8")
        
        soup_subscene = BeautifulSoup(data_subsc_d, 'html.parser')
        contenu_lien_subscene = soup_subscene.find_all('a', {'class':'imdb'})
        lien_imdb = contenu_lien_subscene[0].get("href")
        liste_liens_imdb.append(lien_imdb)

    ## Ajout dans le DataFrame :

    url_title['lien_imdb'] = liste_liens_imdb
    ## Etape 5 done : scraping_url_imdb
    validation_update("scraping_url_imdb")
    
 ## IMDB : ##

    ## Variables locales :

    nom_imdb = ''
    duree_imdb = ''
    note_imdb = ''
    genre_imdb = ''
    image_imdb = ''
    nom_imdb_sansdate = ''

    ## DF Vide :

    resultat_imdb = pd.DataFrame(columns=['Titre','Note','Genres','Duree'])

    ## Boucle sur tous les éléments mis en évidence sur Subscene:

    for i in range(len(url_title)):
        film = url_title.iloc[i,2]

        print(film)

        ## Initialisation driver :

        driver = webdriver.Firefox(options = options, executable_path = path_gecko)

        driver.get(film)
        ## Scroll automatique jusqu'au bas de la page 
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;")
        ## Temporisation de 10 secondes
        time.sleep(10)

        ## Détermination de la version IMDB :
        ## On détecte la version en fonction de la longueur du résultat d'un sélecteur CSS:
        ## Test logique :

        if (len(driver.find_elements(By.CSS_SELECTOR,'h1[data-testid="hero-title-block__title"]')) == 0):

            print("Ancienne version")
            ## Nom du film  :
            find_nom = driver.find_element_by_css_selector(".title_wrapper > h1")
            nom_imdb = find_nom.text.strip()
            ## Etape 6.1 done : scraping_imdb_name
            validation_update("scraping_imdb_name")

            ## Genre(s):
            find_genre = driver.find_element_by_css_selector('.subtext')
            genre_imdb = find_genre.text.split("|")[-2].strip()
            ## Etape 6.2 done : scraping_imdb_genre
            validation_update("scraping_imdb_genre")

            ## Durée du film :
            find_duree = driver.find_element_by_css_selector('.subtext time')
            duree_imdb = find_duree.text.strip()
            ## Etape 6.3 done : scraping_imdb_duration
            validation_update("scraping_imdb_duration")

            ## Note du film :
            find_note = driver.find_element_by_css_selector('.ratingValue span')
            note_imdb = find_note.text.strip()
            ## Etape 6.4 done : scraping_imdb_rating
            validation_update("scraping_imdb_rating")

            driver.quit()

            print(i)

        else:

            print("Nouvelle version")
            ## Nom du film  :               
            find_nom = driver.find_element_by_css_selector('h1[data-testid="hero-title-block__title"]')
            nom_imdb_sansdate = find_nom.text.strip()

            find_date = driver.find_element_by_css_selector('a[href*="releaseinfo?ref_=tt_ov_rdat"]').text

            nom_imdb = nom_imdb_sansdate + " (" + find_date + ")"
            ## Etape 6.1 done : scraping_imdb_name
            validation_update("scraping_imdb_name")

            ## Genre(s):
            find_genre = driver.find_element_by_css_selector("div[data-testid='genres']")
            genre_imdb = find_genre.text.replace("\n",", ")
            ## Etape 6.2 done : scraping_imdb_genre
            validation_update("scraping_imdb_genre")

            ## Durée du film :
            find_duree = driver.find_element_by_css_selector("ul[data-testid='hero-title-block__metadata'] > li:last-child")
            duree_imdb = find_duree.text.strip()
            ## Etape 6.3 done : scraping_imdb_duration
            validation_update("scraping_imdb_duration")

            ## Note du film :

            if (len(driver.find_elements(By.CSS_SELECTOR,"div[data-testid='hero-rating-bar__aggregate-rating__score'] span")) >0):
                find_note = driver.find_element_by_css_selector("div[data-testid='hero-rating-bar__aggregate-rating__score'] span")
                note_imdb = find_note.text.strip()
            else:
                note_imdb = "N/A"

            ## Etape 6.4 done : scraping_imdb_rating
            validation_update("scraping_imdb_rating")

            driver.quit()

            print(i)

        ## Dans le DF :
        row= {'Titre': nom_imdb, 'Note' : note_imdb , 'Genres' : genre_imdb,'Duree' : duree_imdb}
        resultat_imdb = resultat_imdb.append(row,ignore_index=True)

        resultat_imdb['Lien_IMDB'] = url_title['lien_imdb']
        resultat_imdb['Lien_Subscene'] = url_title['Url']
    
    return resultat_imdb

## Appel ##

### Initialisation du fichier log

columns_logs = ['date','scraping_subscene','parsing_subscene','filtres_cam','url_title','scraping_url_imdb','scraping_imdb_name','scraping_imdb_genre','scraping_imdb_duration','scraping_imdb_rating','film_push','push_archive','email_sent']
try :
    archive_log = pd.read_csv(filepath_or_buffer = "archive_log.csv", sep=";", index_col= None, usecols=columns_logs)
except :
    archive_log = pd.DataFrame(columns = columns_logs)

date_new_entry = date.today().strftime('%d/%m/%Y')

new_entry_log = {'date':date_new_entry,'scraping_subscene':0,'parsing_subscene':0,'filtres_cam':0,'url_title':0,'scraping_url_imdb':0,
               'scraping_imdb_name':0,'scraping_imdb_genre':0,'scraping_imdb_duration':0,'scraping_imdb_rating':0,
               'film_push':0,'push_archive':0,'email_sent':0}

archive_log_push = archive_log.append(new_entry_log,ignore_index=True)

archive_log_push.to_csv(path_or_buf= "archive_log.csv", sep = ";", index = False)

### Lancement Scraping 

tableau_resultat = scrap_all()


## ~~ Film Push et Archive ~~ ##

film_push = pd.DataFrame(columns=['Titre','Note','Genres','Duree','Lien_IMDB','Lien_Subscene'])

try :
    archive_imdb = pd.read_csv(filepath_or_buffer= "archive_imdb.csv", sep=";", index_col= None, usecols=['Titre','Note','Genres','Duree','Lien_IMDB','Lien_Subscene'])
except :
    archive_imdb = pd.DataFrame(columns=['Titre','Note','Genres','Duree','Lien_IMDB','Lien_Subscene'])

if archive_imdb.empty :
    film_push = tableau_resultat
else :
    for j in range(len(tableau_resultat)):
        film_scrape = tableau_resultat.iloc[j,4]
        if film_scrape not in list(archive_imdb['Lien_IMDB']):
            film_push = film_push.append(tableau_resultat.iloc[j,:])

## Etape 7 done : film_push
validation_update("film_push")

## ~~ Mise en archive  ~~ ##

nouvelle_archive = pd.concat([archive_imdb,film_push], axis = 0)
nouvelle_archive.reset_index(inplace = True, drop = True)
nouvelle_archive.to_csv(path_or_buf= "archive_imdb.csv", sep = ";", index = False)
nouvelle_archive.to_csv(path_or_buf= "archive_imdb_{}.csv".format(today.strftime('%d-%m-%Y')), sep = ";", index = False)


film_push.reset_index(inplace = True, drop = True)

## Etape 8 done : push_archive
validation_update("push_archive")

## ~~ Contenu du mail ~~ ##
html_essai = """\
<html>
  <head></head>
  <body>
  <h2> Voici les films de la semaine :</h2>
    {0} \n
  <h2> Voici les logs :</h2>
    {1}
  </body>
</html>
""".format(film_push.to_html(),archive_log_push.to_html())


## ~~ PUSH Sendgrid ~~ ##

message = Mail(
    from_email='****',
    to_emails=['****', '****','****'],
    subject="Update IMDB - {}".format(today.strftime('%d/%m/%Y')),
    html_content=html_essai)

with open('archive_imdb.csv', 'rb') as f:
    data = f.read()
    f.close()
encoded_file = base64.b64encode(data).decode()

attachedFile1 = Attachment(
    FileContent(encoded_file),
    FileName('archive_imdb_{}.csv'.format(today.strftime('%d-%m-%Y'))),
    FileType('application/csv'),
    Disposition('attachment')
)

attachedFile2 = Attachment(
    FileContent(encoded_file),
    FileName('archive_log.csv'),
    FileType('application/csv'),
    Disposition('attachment')
)
message.attachment = [attachedFile1,attachedFile2]

sg = SendGridAPIClient('*****')
response = sg.send(message)

## Etape 9 done : email_sent
validation_update("email_sent")
